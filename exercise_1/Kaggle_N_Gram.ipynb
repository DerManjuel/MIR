{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DerManjuel/MIR/blob/Manuel/Kaggle_N_Gram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"display:fill;\n",
        "            border-radius:15px;\n",
        "            background-color:skyblue;\n",
        "            font-size:210%;\n",
        "            letter-spacing:0.5px;\n",
        "            padding:10px;\n",
        "            color:white;\n",
        "            border-style: solid;\n",
        "            border-color: black;\n",
        "            text-align:center;\">\n",
        "<b>\n",
        " ‚öïÔ∏èüî¨ Text Classification using N-Gram Feature Extraction üìù\n"
      ],
      "metadata": {
        "id": "1cfxS-Mo8YF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "\n",
        "Medical domain is in a data rich environment that a variety of knowledge can be extracted for positive outcomes. This notebook work will show multiclass classification of medical transcriptions using a real dataset. The objective of this paper is to classify medical transcriptions based on the medical specialty labels, namely Discharge Summary, Neurosurgery and ENT. Text normalisation has performed followed by extracting five different n-gram feature representations are. Moreover, three supervised learning classifiers were trained on each of the n-gram feature representations, namely K-Nearest Neighbours, Decision Tree, and Random Forest. The classification performance was evaluated by the metric score of macro F1. The best score achieved was over 0.8 macro F1 on testing set using tuned Random Forest and unigram feature vectors.\n",
        "\n",
        "**Notebook Author**\n",
        "\n",
        "Morris Lee 14/9/2022\n",
        "\n",
        "\n",
        "**Concept of N-Gram**\n",
        "\n",
        "In this notebook, n-gram will be used for feature extraction, which is the first NLP approach that introduced by Markov in 1913 [1]. An N-gram is an N-character slice of a longer string. The intuition of the n-gram model is that instead of computing a prediction based on entire corpus, one can approximate the prediction by only contiguous slices sequence of n words [2]. \n",
        "To explain feature extraction using n-gram with a demonstration of the sentence, ‚ÄúThe student is alone happily‚Äù. The number of n-gram features can be calculated by k-n+1, where k is the number of words. The result is a bag-of-n-grams model [3] for a classifier to train the linguistic algorithm. Table I below shows the demonstration of different n-gram feature representations.\n",
        "\n",
        "![image.png](attachment:68f3d7d5-9233-4338-b01f-764bf072d9a1.png)"
      ],
      "metadata": {
        "id": "OIqDOi0V8YF6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Contents**\n",
        "\n",
        "#### [1.0 Import Functions and EDA](#1.0)\n",
        "* [1.1 Import Functions](#1.1)\n",
        "* [1.2 Word Counts of Each Medical Specialty](#1.2)\n",
        "* [1.3 Sample Size of Each Medical Specialty](#1.3)\n",
        "* [1.4 General Cleaning](#1.4)\n",
        "\n",
        "#### [2.0 Text Normalisation](#2.0)\n",
        "* [2.1 Lower Case](#2.1)\n",
        "* [2.2 Remove Punctuation and Numbers](#2.2)\n",
        "* [2.3 Tokenisation](#2.3)\n",
        "* [2.4 Stemming ](#2.4)\n",
        "\n",
        "#### [3.0 Text N-Gram Feature Extraction](#3.0)\n",
        "* [3.1 Extract 5 Types of N-Gram](#3.1)\n",
        "* [3.2 Dimension of Each Feature Vector](#3.2)\n",
        "\n",
        "#### [4.0 Text Classification Modelling](#4.0)\n",
        "* [4.1 Visualising Classification Prediction](#4.1)\n",
        "* [4.2 Dimensionality Reduction](#4.2)\n",
        "* [4.3 Obtain Best Classifier and Feature Vector](#4.3)\n",
        "* [4.4 Evaluate on Each Class Labels](#4.4)"
      ],
      "metadata": {
        "id": "C2F3TOCj8YF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import warnings\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import os"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T13:36:12.855107Z",
          "iopub.execute_input": "2022-09-14T13:36:12.855577Z",
          "iopub.status.idle": "2022-09-14T13:36:14.183374Z",
          "shell.execute_reply.started": "2022-09-14T13:36:12.855542Z",
          "shell.execute_reply": "2022-09-14T13:36:14.181741Z"
        },
        "trusted": true,
        "id": "UkRav8bx8YF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trim(df):\n",
        "    df.columns = df.columns.str.strip()\n",
        "    df = df.drop_duplicates()\n",
        "    df.columns = df.columns.str.lower()\n",
        "    df.columns = df.columns.str.replace(' ','_')\n",
        "    df_obj = df.select_dtypes(['object'])\n",
        "    df[df_obj.columns] = df_obj.apply(lambda x: x.str.strip())\n",
        "    print(\"All column names have been striped, lowered case, replaced space with underscore if any\")\n",
        "    print(\"Dropped duplicated instances if any\")\n",
        "    print(\"Categorical instances have been striped\")\n",
        "    return df\n",
        "\n",
        "pd.set_option('display.max_colwidth', 255)\n",
        "df =pd.read_csv('/kaggle/input/medicaltranscriptions/mtsamples.csv')\n",
        "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "df = trim(df)\n",
        "\n",
        "def vc(df, column, r=False):\n",
        "    vc_df = df.reset_index().groupby([column]).size().to_frame('count')\n",
        "    vc_df['percentage (%)'] = vc_df['count'].div(sum(vc_df['count'])).mul(100)\n",
        "    vc_df = vc_df.sort_values(by=['percentage (%)'], ascending=False)\n",
        "    if r:\n",
        "        return vc_df\n",
        "    else:\n",
        "        print(f'STATUS: Value counts of \"{column}\"...')\n",
        "        display(vc_df)\n",
        "        \n",
        "def shape(df,df_name):\n",
        "    print(f'STATUS: Dimension of \"{df_name}\" = {df.shape}')\n",
        "        \n",
        "df.head(3)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T13:36:47.318638Z",
          "iopub.execute_input": "2022-09-14T13:36:47.319145Z",
          "iopub.status.idle": "2022-09-14T13:36:47.777014Z",
          "shell.execute_reply.started": "2022-09-14T13:36:47.319105Z",
          "shell.execute_reply": "2022-09-14T13:36:47.775786Z"
        },
        "trusted": true,
        "id": "SsnAcHQn8YF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df['medical_specialty'].isin(['Neurosurgery','ENT - Otolaryngology','Discharge Summary'])]\n",
        "shape(df,'df')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T13:37:06.84697Z",
          "iopub.execute_input": "2022-09-14T13:37:06.848259Z",
          "iopub.status.idle": "2022-09-14T13:37:06.85781Z",
          "shell.execute_reply.started": "2022-09-14T13:37:06.848203Z",
          "shell.execute_reply": "2022-09-14T13:37:06.855985Z"
        },
        "trusted": true,
        "id": "VQ2xWN2_8YF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>1.2 <span style='color:red'>|</span> Word Counts of Each Medical Specialty </b> <a class=\"anchor\" id=\"1.2\"></a>"
      ],
      "metadata": {
        "id": "e1-_J15U8YF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To query the data, I would like to know how is the size of the dataset and also to rank null values in descending order"
      ],
      "metadata": {
        "id": "pOAOhiET8YF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "medical_specialty_list = [] ; word_count_list =[]\n",
        "for medical_specialty in df['medical_specialty'].unique():\n",
        "    df_filter = df.loc[(df['medical_specialty'] == medical_specialty)]\n",
        "    word_count_temp = df_filter['transcription'].str.split().str.len().sum()\n",
        "    medical_specialty_list.append(medical_specialty)\n",
        "    word_count_list.append(word_count_temp)\n",
        "word_count_df = pd.DataFrame({'Medical Specialty':medical_specialty_list, 'Word Count':word_count_list})\n",
        "word_count_df['Word Count'] = word_count_df['Word Count'].astype('int')\n",
        "word_count_df = word_count_df.sort_values('Word Count', ascending=False)\n",
        "word_count_df.reset_index(drop=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T13:37:23.08986Z",
          "iopub.execute_input": "2022-09-14T13:37:23.091006Z",
          "iopub.status.idle": "2022-09-14T13:37:23.139325Z",
          "shell.execute_reply.started": "2022-09-14T13:37:23.090957Z",
          "shell.execute_reply": "2022-09-14T13:37:23.137769Z"
        },
        "trusted": true,
        "id": "EXRJstwO8YF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_word_count = df['transcription'].str.split().str.len().sum()\n",
        "print(f'The word count of all transcription is: {int(total_word_count)}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T13:37:29.352596Z",
          "iopub.execute_input": "2022-09-14T13:37:29.35303Z",
          "iopub.status.idle": "2022-09-14T13:37:29.373219Z",
          "shell.execute_reply.started": "2022-09-14T13:37:29.352963Z",
          "shell.execute_reply": "2022-09-14T13:37:29.372252Z"
        },
        "trusted": true,
        "id": "szMtRXUX8YF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>1.3 <span style='color:red'>|</span> Sample Size of Each Medical Specialty </b> <a class=\"anchor\" id=\"1.3\"></a>"
      ],
      "metadata": {
        "id": "ECH9DuMg8YF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vc(df, 'medical_specialty')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T13:37:39.337085Z",
          "iopub.execute_input": "2022-09-14T13:37:39.337524Z",
          "iopub.status.idle": "2022-09-14T13:37:39.356645Z",
          "shell.execute_reply.started": "2022-09-14T13:37:39.33749Z",
          "shell.execute_reply": "2022-09-14T13:37:39.355788Z"
        },
        "trusted": true,
        "id": "A1e0kx5t8YF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>1.4 <span style='color:red'>|</span> General Cleaning </b> <a class=\"anchor\" id=\"1.4\"></a>"
      ],
      "metadata": {
        "id": "m3oLt3bG8YF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to print data shape\n",
        "print(f'data shape is: {df.shape}')\n",
        "\n",
        "# to identify the null values by descending order\n",
        "df.isnull().sum().sort_values(ascending = False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T13:37:49.897056Z",
          "iopub.execute_input": "2022-09-14T13:37:49.89751Z",
          "iopub.status.idle": "2022-09-14T13:37:49.909494Z",
          "shell.execute_reply.started": "2022-09-14T13:37:49.897476Z",
          "shell.execute_reply": "2022-09-14T13:37:49.90855Z"
        },
        "trusted": true,
        "id": "DVAZ6Oo88YF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One important detail is that I found out there are 2 rows containing no transcription. They should be removed as transcription is our only predictors in this text classification task."
      ],
      "metadata": {
        "id": "D_INhxPm8YF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to remove transcription rows that is empty\n",
        "df = df[df['transcription'].notna()]\n",
        "df.info()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T13:37:59.879333Z",
          "iopub.execute_input": "2022-09-14T13:37:59.879732Z",
          "iopub.status.idle": "2022-09-14T13:37:59.911263Z",
          "shell.execute_reply.started": "2022-09-14T13:37:59.879702Z",
          "shell.execute_reply": "2022-09-14T13:37:59.909782Z"
        },
        "trusted": true,
        "id": "WgFSWsig8YF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After dropping the null values, there are no null values for the transcription attribute. "
      ],
      "metadata": {
        "id": "jGCOkfR48YF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# drop redundant columns\n",
        "df =df.drop(['description','sample_name','keywords'], axis=1)\n",
        "df.head(2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T13:38:25.765836Z",
          "iopub.execute_input": "2022-09-14T13:38:25.766311Z",
          "iopub.status.idle": "2022-09-14T13:38:25.782036Z",
          "shell.execute_reply.started": "2022-09-14T13:38:25.766276Z",
          "shell.execute_reply": "2022-09-14T13:38:25.78017Z"
        },
        "trusted": true,
        "id": "LQPJm8S68YF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The target labels (or the topic) is the 'medical_specialty' attribute. Now, let's identify how is the value counts of the target labels, and as well visualise it in a bar chart. In order to visualise in matplotlib, function of flattening list is defined in order to put the target value counts into the matplotlib function."
      ],
      "metadata": {
        "id": "NVQ6XNFG8YF9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The target labels is quite balanced"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T13:38:45.540435Z",
          "iopub.execute_input": "2022-09-14T13:38:45.540923Z",
          "iopub.status.idle": "2022-09-14T13:38:45.548809Z",
          "shell.execute_reply.started": "2022-09-14T13:38:45.540883Z",
          "shell.execute_reply": "2022-09-14T13:38:45.546804Z"
        },
        "id": "_rBcUbR_8YF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>2.0 <span style='color:red'>|</span> Text Normalisation </b> <a class=\"anchor\" id=\"2.0\"></a>"
      ],
      "metadata": {
        "id": "Z7q--rgM8YF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data normalisation will be conducted for the trascription. One of the reasons is to convert the transcript into standard format, which important for data extraction later. In this data normalisation task, following task will be executed, which are:\n",
        "1. Lowe Case\n",
        "2. Removing punctuation and numbers\n",
        "3. Tokenisation of the transcription\n",
        "4. Lemmatisation\n",
        "5. Remove Stop Words"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T13:39:00.032363Z",
          "iopub.execute_input": "2022-09-14T13:39:00.032844Z",
          "iopub.status.idle": "2022-09-14T13:39:00.043819Z",
          "shell.execute_reply.started": "2022-09-14T13:39:00.032806Z",
          "shell.execute_reply": "2022-09-14T13:39:00.041823Z"
        },
        "id": "CeSAcyOS8YF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>2.1 <span style='color:red'>|</span> Lower Case </b> <a class=\"anchor\" id=\"2.1\"></a>"
      ],
      "metadata": {
        "id": "exp_Mvqh8YF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To convert transcription into lowercase\n",
        "def lower(df, attribute):\n",
        "    df.loc[:,attribute] = df[attribute].apply(lambda x : str.lower(x))\n",
        "    return df\n",
        "df = lower(df,'transcription')\n",
        "df.head(3)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T13:39:17.00863Z",
          "iopub.execute_input": "2022-09-14T13:39:17.01001Z",
          "iopub.status.idle": "2022-09-14T13:39:17.030127Z",
          "shell.execute_reply.started": "2022-09-14T13:39:17.009955Z",
          "shell.execute_reply": "2022-09-14T13:39:17.028704Z"
        },
        "trusted": true,
        "id": "b5XbIO4N8YF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>2.2 <span style='color:red'>|</span> Remove Punctuation and Numbers </b> <a class=\"anchor\" id=\"2.2\"></a>"
      ],
      "metadata": {
        "id": "b9yTxAjr8YF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To remove transcription punctuation and numbers\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "def remove_punc_num(df, attribute):\n",
        "    df.loc[:,attribute] = df[attribute].apply(lambda x : \" \".join(re.findall('[\\w]+',x)))\n",
        "    df[attribute] = df[attribute].str.replace('\\d+', '')\n",
        "    return df\n",
        "df =remove_punc_num(df, 'transcription')\n",
        "df_no_punc =df.copy()\n",
        "df.head(3)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T13:39:27.452849Z",
          "iopub.execute_input": "2022-09-14T13:39:27.453428Z",
          "iopub.status.idle": "2022-09-14T13:39:27.54031Z",
          "shell.execute_reply.started": "2022-09-14T13:39:27.453387Z",
          "shell.execute_reply": "2022-09-14T13:39:27.539122Z"
        },
        "trusted": true,
        "id": "ElqlxExW8YF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>2.3 <span style='color:red'>|</span> Tokenisation </b> <a class=\"anchor\" id=\"2.3\"></a>\n"
      ],
      "metadata": {
        "id": "Idjz-_k58YF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to tokenise transcription\n",
        "\n",
        "# import nltk\n",
        "tk =WhitespaceTokenizer()\n",
        "def tokenise(df, attribute):\n",
        "    df['tokenised'] = df.apply(lambda row: tk.tokenize(str(row[attribute])), axis=1)\n",
        "    return df\n",
        "df =tokenise(df, 'transcription')\n",
        "df_experiment =df.copy()\n",
        "df.head(3)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T13:39:38.130949Z",
          "iopub.execute_input": "2022-09-14T13:39:38.131698Z",
          "iopub.status.idle": "2022-09-14T13:39:38.218594Z",
          "shell.execute_reply.started": "2022-09-14T13:39:38.131633Z",
          "shell.execute_reply": "2022-09-14T13:39:38.217159Z"
        },
        "trusted": true,
        "id": "sillY6JY8YF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>2.4 <span style='color:red'>|</span> Stemming </b> <a class=\"anchor\" id=\"2.4\"></a>"
      ],
      "metadata": {
        "id": "pwNJIKTA8YF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "def stemming(df, attribute):\n",
        "    # Use English stemmer.\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    df['stemmed'] = df[attribute].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.\n",
        "    return df\n",
        "df =stemming(df_experiment, 'tokenised')\n",
        "df.head(2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T13:39:54.904181Z",
          "iopub.execute_input": "2022-09-14T13:39:54.904614Z",
          "iopub.status.idle": "2022-09-14T13:39:56.844398Z",
          "shell.execute_reply.started": "2022-09-14T13:39:54.90458Z",
          "shell.execute_reply": "2022-09-14T13:39:56.84308Z"
        },
        "trusted": true,
        "id": "ok2Z_5oG8YF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>2.5 <span style='color:red'>|</span> Stop Words Removal </b> <a class=\"anchor\" id=\"2.5\"></a>\n",
        "\n",
        "\n",
        "Removing stop words from the feature space, otherwise it will affect the classifier performance as the collection frequency is often high"
      ],
      "metadata": {
        "id": "O3RQ6dKY8YF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Showing the list of the English stop words, it has a number of 179 stop words in this list\n",
        "\n",
        "stop = stopwords.words('english')\n",
        "print(f\"There are {len(stop)} stop words \\n\")\n",
        "print(stop)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T13:40:32.264511Z",
          "iopub.execute_input": "2022-09-14T13:40:32.265195Z",
          "iopub.status.idle": "2022-09-14T13:40:32.283508Z",
          "shell.execute_reply.started": "2022-09-14T13:40:32.265131Z",
          "shell.execute_reply": "2022-09-14T13:40:32.281877Z"
        },
        "trusted": true,
        "id": "x8oI7pul8YF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing stop words\n",
        "def remove_stop_words(df, attribute):\n",
        "    stop = stopwords.words('english')\n",
        "    df['stemmed_without_stop'] = df[attribute].apply(lambda x: ' '.join([word for word in x if word not in (stop)]))\n",
        "    return df\n",
        "df = remove_stop_words(df, 'stemmed')\n",
        "df.head(2)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T13:40:44.751378Z",
          "iopub.execute_input": "2022-09-14T13:40:44.751876Z",
          "iopub.status.idle": "2022-09-14T13:40:45.10162Z",
          "shell.execute_reply.started": "2022-09-14T13:40:44.75184Z",
          "shell.execute_reply": "2022-09-14T13:40:45.100426Z"
        },
        "trusted": true,
        "id": "u9V6zwDx8YF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the 5 data normalisation steps, each transcription record is now in a standard format, which is ready for the n-gram features extraction later. Hence, we should use the attribute 'stemmed_withou_stop' as the predictor attribute and drop other redundant attributes, namely 'transcription', 'tokenized_transcription' and 'stemmed'."
      ],
      "metadata": {
        "id": "I7m2bsl68YF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df =df.drop(['transcription','stemmed', 'tokenised'], axis=1)\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T13:40:59.741647Z",
          "iopub.execute_input": "2022-09-14T13:40:59.74223Z",
          "iopub.status.idle": "2022-09-14T13:40:59.758509Z",
          "shell.execute_reply.started": "2022-09-14T13:40:59.742188Z",
          "shell.execute_reply": "2022-09-14T13:40:59.756903Z"
        },
        "trusted": true,
        "id": "FTDuuBRM8YF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_word_count_normalised = df['stemmed_without_stop'].str.split().str.len().sum()\n",
        "print(f'The word count of transcription after normalised is: {int(total_word_count_normalised)}')\n",
        "print(f'{round((total_word_count - total_word_count_normalised)/total_word_count*100, 2)}% less word')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T13:41:14.136791Z",
          "iopub.execute_input": "2022-09-14T13:41:14.137771Z",
          "iopub.status.idle": "2022-09-14T13:41:14.160706Z",
          "shell.execute_reply.started": "2022-09-14T13:41:14.137708Z",
          "shell.execute_reply": "2022-09-14T13:41:14.15921Z"
        },
        "trusted": true,
        "id": "-Ti4nT1k8YF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(df['medical_specialty'])\n",
        "df['encoded_target'] = le.transform(df['medical_specialty'])\n",
        "df.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T13:41:19.727497Z",
          "iopub.execute_input": "2022-09-14T13:41:19.727968Z",
          "iopub.status.idle": "2022-09-14T13:41:19.743843Z",
          "shell.execute_reply.started": "2022-09-14T13:41:19.727931Z",
          "shell.execute_reply": "2022-09-14T13:41:19.742474Z"
        },
        "trusted": true,
        "id": "vJsEP2vM8YGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>3.0 <span style='color:red'>|</span> Text N-Gram Feature Extraction </b> <a class=\"anchor\" id=\"3.0\"></a>"
      ],
      "metadata": {
        "id": "ucOd4vKS8YGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use sklearn class 'CountVectoriser' to extract different n-grams features. In order to do so, the transcription should be converted into a list format, rather than a dataframe. For the purpose of converting into a flat list (i.e., there is no inner list), the function of 'flat_list' that defined above is used."
      ],
      "metadata": {
        "id": "0gxOb4988YGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to flatten one list\n",
        "def flat_list(unflat_list):\n",
        "    flatted = [item for sublist in unflat_list for item in sublist]\n",
        "    return flatted\n",
        "\n",
        "def to_list(df, attribute):\n",
        "    # Select the normalised transcript column \n",
        "    df_transcription = df[[attribute]]\n",
        "    # To convert the attribute into list format, but it has inner list. So it cannot put into the CountVectoriser\n",
        "    unflat_list_transcription = df_transcription.values.tolist()\n",
        "    # Let's use back the function defined above, \"flat_list\", to flatten the list\n",
        "    flat_list_transcription = flat_list(unflat_list_transcription)\n",
        "    return flat_list_transcription\n",
        "flat_list_transcription = to_list(df, 'stemmed_without_stop')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T13:41:35.600193Z",
          "iopub.execute_input": "2022-09-14T13:41:35.600608Z",
          "iopub.status.idle": "2022-09-14T13:41:35.609558Z",
          "shell.execute_reply.started": "2022-09-14T13:41:35.600575Z",
          "shell.execute_reply": "2022-09-14T13:41:35.608157Z"
        },
        "trusted": true,
        "id": "61WA96zB8YGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>3.1 <span style='color:red'>|</span> Extract 5 Types of N-Gram </b> <a class=\"anchor\" id=\"3.1\"></a>\n"
      ],
      "metadata": {
        "id": "d96Fj-Im8YGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CountVectorizer is used to convert a collection of transcript documents to a matrix of n-gram features. To explain the ngram_range, all values of n such such that min_n <= n <= max_n will be used. For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams."
      ],
      "metadata": {
        "id": "Qm3wI3vh8YGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_gram_features ={'unigram':(1,1),'unigram_bigram':(1,2),'bigram':(2,2),\\\n",
        "       'bigram_trigram':(2,3),'trigram':(3,3)}\n",
        "feature_name=[]\n",
        "temp=[]\n",
        "for key, values in n_gram_features.items():\n",
        "    temp.append(key)\n",
        "    feature_name.append(key)\n",
        "temp"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T13:41:52.40424Z",
          "iopub.execute_input": "2022-09-14T13:41:52.404798Z",
          "iopub.status.idle": "2022-09-14T13:41:52.417151Z",
          "shell.execute_reply.started": "2022-09-14T13:41:52.404757Z",
          "shell.execute_reply": "2022-09-14T13:41:52.415884Z"
        },
        "trusted": true,
        "id": "Wdf-E03-8YGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_n_gram_features(flat_list_transcription):\n",
        "    temp=[]\n",
        "    for key, values in n_gram_features.items(): \n",
        "        vectorizer = CountVectorizer(ngram_range=values)\n",
        "        vectorizer.fit(flat_list_transcription)\n",
        "        temp.append(vectorizer.transform(flat_list_transcription))\n",
        "    return temp\n",
        "temp = generate_n_gram_features(flat_list_transcription)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T13:41:57.391751Z",
          "iopub.execute_input": "2022-09-14T13:41:57.392223Z",
          "iopub.status.idle": "2022-09-14T13:41:59.530759Z",
          "shell.execute_reply.started": "2022-09-14T13:41:57.392188Z",
          "shell.execute_reply": "2022-09-14T13:41:59.529531Z"
        },
        "trusted": true,
        "id": "2BaLil-98YGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>3.2 <span style='color:red'>|</span> Dimension of Each Feature Vector </b> <a class=\"anchor\" id=\"3.2\"></a>"
      ],
      "metadata": {
        "id": "IccAx9Bc8YGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataframes = {'unigram':temp[0], \n",
        "              'unigram_bigram':temp[1], \n",
        "              'bigram':temp[2], \n",
        "              'bigram_trigram':temp[3], \n",
        "              'trigram':temp[4]}\n",
        "feature_vector = [] ; feature_vector_shape = []\n",
        "for key in dataframes:\n",
        "    feature_vector.append(key)\n",
        "    feature_vector_shape.append(dataframes[key].shape)\n",
        "\n",
        "n_gram_df = pd.DataFrame({'N-Gram Feature Vector':feature_vector, 'Data Dimension':feature_vector_shape})\n",
        "n_gram_df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T13:42:07.444408Z",
          "iopub.execute_input": "2022-09-14T13:42:07.444917Z",
          "iopub.status.idle": "2022-09-14T13:42:07.465441Z",
          "shell.execute_reply.started": "2022-09-14T13:42:07.44488Z",
          "shell.execute_reply": "2022-09-14T13:42:07.464117Z"
        },
        "trusted": true,
        "id": "78U_Lo2X8YGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the feature extraction process, 5 kinds of n-gram features are extracted. It is interesting to notice that when the number of 'n' getting higher (i.e, n=1:unigram, n=2:bigram, n=3:trigram), there is a higer number of columns. This is due to it is getting harder to find similar features that can be stored in similar column when it has a longer connected words as one featuer. If the feature is unique, it will automatically append additional column to store the feaure."
      ],
      "metadata": {
        "id": "4wdpT_f_8YGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to retrieve a unigram feature vector\n",
        "dataframes['unigram']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T13:42:19.759599Z",
          "iopub.execute_input": "2022-09-14T13:42:19.760015Z",
          "iopub.status.idle": "2022-09-14T13:42:19.768463Z",
          "shell.execute_reply.started": "2022-09-14T13:42:19.759983Z",
          "shell.execute_reply": "2022-09-14T13:42:19.767157Z"
        },
        "trusted": true,
        "id": "_Tz1Mh6s8YGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>4.0 <span style='color:red'>|</span> Text Classification Modelling </b> <a class=\"anchor\" id=\"4.0\"></a>"
      ],
      "metadata": {
        "id": "Vc9voLBl8YGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
        "from sklearn.model_selection import HalvingGridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "random_state_number =8888\n",
        "df_target =df[['encoded_target']].values.ravel()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T13:42:36.762674Z",
          "iopub.execute_input": "2022-09-14T13:42:36.763216Z",
          "iopub.status.idle": "2022-09-14T13:42:36.933826Z",
          "shell.execute_reply.started": "2022-09-14T13:42:36.763171Z",
          "shell.execute_reply": "2022-09-14T13:42:36.932473Z"
        },
        "trusted": true,
        "id": "UPUF5Lu-8YGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = {\n",
        "    'f1':[f1_score, 'f1_macro'], \n",
        "    'precision': [precision_score, 'precision_macro'], \n",
        "    'recall': [recall_score, 'recall_macro']\n",
        "}\n",
        "\n",
        "# get evaluation result\n",
        "\n",
        "def get_performance(param_grid, base_estimator, dataframes):\n",
        "    df_name_list =[]; best_estimator_list=[]; best_score_list=[]; test_predict_result_list=[];\n",
        "    metric_list = [];\n",
        "    \n",
        "    for df_name, df in dataframes.items():\n",
        "        \n",
        "        X_train, X_test, y_train, y_test = train_test_split(df, df_target, test_size=0.2, random_state=random_state_number)\n",
        "        for _, metric_dict in metrics.items():\n",
        "            sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5, scoring=metric_dict[1],random_state=random_state_number,\n",
        "                                      factor=2).fit(X_train, y_train)\n",
        "\n",
        "            best_estimator = sh.best_estimator_\n",
        "            clf = best_estimator.fit(X_train, y_train)\n",
        "            prediction = clf.predict(X_test)\n",
        "            test_predict_result = metric_dict[0](y_test, prediction, average='macro')\n",
        "\n",
        "            df_name_list.append(df_name) ; best_estimator_list.append(best_estimator) ; \n",
        "            best_score_list.append(sh.best_score_) ; \n",
        "            test_predict_result_list.append(test_predict_result) ;metric_list.append(metric_dict[1])\n",
        "            \n",
        "            \n",
        "    model_result = pd.DataFrame({'Vector':df_name_list,'Metric':metric_list,\n",
        "                               'Calibrated Estimator':best_estimator_list,\n",
        "                               'Best CV Metric Score':best_score_list, 'Test Predict Metric Score': test_predict_result_list})\n",
        "    return model_result\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T13:42:41.377863Z",
          "iopub.execute_input": "2022-09-14T13:42:41.378344Z",
          "iopub.status.idle": "2022-09-14T13:42:41.391561Z",
          "shell.execute_reply.started": "2022-09-14T13:42:41.378301Z",
          "shell.execute_reply": "2022-09-14T13:42:41.390141Z"
        },
        "trusted": true,
        "id": "-NVYeFoP8YGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>4.1 <span style='color:red'>|</span> Visualising Classification Prediction </b> <a class=\"anchor\" id=\"4.1\"></a>"
      ],
      "metadata": {
        "id": "UUcnqABT8YGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "font = {'family' : 'Tahoma',\n",
        "        'weight' : 'bold',\n",
        "        'size'   : 12}\n",
        "matplotlib.rc('font', **font)\n",
        "\n",
        "def vis_classification(vector_type = 'unigram', estimator = KNeighborsClassifier(n_neighbors=9)):\n",
        "    pca = PCA(n_components=2)\n",
        "    df1 = pca.fit_transform(dataframes[vector_type].todense())\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df1, df_target, test_size=0.2, random_state=random_state_number)\n",
        "    \n",
        "    # get training set\n",
        "    df2 = pd.DataFrame({'pca1':X_train[:,1], 'pca2': X_train[:,0], 'y':le.inverse_transform(y_train)})\n",
        "    min_1, max_1 = df2['pca1'].min(), df2['pca1'].max()\n",
        "    min_2, max_2 = df2['pca2'].min(), df2['pca2'].max()\n",
        "    \n",
        "    # generate dimension reduced, but extended data\n",
        "    pca1_range = np.linspace(min_1,max_1,30)\n",
        "    pca2_range = np.linspace(min_2,max_2,30)\n",
        "    \n",
        "    # shuffle\n",
        "    np.random.shuffle(pca1_range) ; np.random.shuffle(pca2_range)\n",
        "    \n",
        "    # to dataframe\n",
        "    prediction_test = pd.DataFrame({'pca1':pca1_range, 'pca2':pca2_range})\n",
        "\n",
        "    best_estimator = estimator\n",
        "    \n",
        "    # fit training set and predict extended data\n",
        "    clf = best_estimator.fit(X_train, y_train)\n",
        "\n",
        "    fig, axs = plt.subplots(nrows = 1, ncols = 2, figsize=(15,6))\n",
        "    cmap = plt.cm.get_cmap('tab10', 4)\n",
        "    fig.suptitle(f\"Visualising {type(estimator).__name__} on {vector_type.capitalize()} Vector\", fontsize=14,fontweight='bold')\n",
        "\n",
        "\n",
        "    def plot_scatter(ax, predictor_set, target, title):\n",
        "        \n",
        "        # plot area classifier\n",
        "        clf = best_estimator.fit(X_train, y_train)\n",
        "        axs[0].tricontourf(X_train[:,0], X_train[:,1], clf.predict(X_train), levels=np.arange(-0.5, 4), zorder=10, alpha=0.3, cmap=cmap, edgecolors=\"k\")\n",
        "        \n",
        "        axs[1].tricontourf(X_test[:,0], X_test[:,1], clf.predict(X_test), levels=np.arange(-0.5, 4), zorder=10, alpha=0.3, cmap=cmap, edgecolors=\"k\")\n",
        "        \n",
        "        # plot scatter\n",
        "        df3 = pd.DataFrame({'pca1':predictor_set[:,1], 'pca2': predictor_set[:,0], 'y':le.inverse_transform(target)})\n",
        "        for y_label in df3['y'].unique():\n",
        "            df_filter = df3[df3['y']==y_label]\n",
        "            ax.scatter(df_filter['pca1'], df_filter['pca2'], alpha=1,label=f\"{y_label}\")\n",
        "        ax.legend()\n",
        "        ax.set_title(f'{title} ({predictor_set.shape[0]} Samples)',fontweight='bold')\n",
        "    plot_scatter(axs[0], X_train, y_train, 'Training Set')\n",
        "    plot_scatter(axs[1], X_test, y_test, 'Testing Set')\n",
        "    axs[0].sharey(axs[1])\n",
        "    return plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T13:42:52.544308Z",
          "iopub.execute_input": "2022-09-14T13:42:52.54486Z",
          "iopub.status.idle": "2022-09-14T13:42:52.564559Z",
          "shell.execute_reply.started": "2022-09-14T13:42:52.544804Z",
          "shell.execute_reply": "2022-09-14T13:42:52.563136Z"
        },
        "trusted": true,
        "id": "8VftEkd18YGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'max_depth': [None,30,32,35,37,38,39,40],'min_samples_split': [2,150,170,180,190,200]}\n",
        "base_estimator = RandomForestClassifier(random_state=random_state_number)\n",
        "rfc_result = get_performance(param_grid, base_estimator, dataframes)\n",
        "rfc_result"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T13:42:59.030224Z",
          "iopub.execute_input": "2022-09-14T13:42:59.031752Z",
          "iopub.status.idle": "2022-09-14T14:02:34.163875Z",
          "shell.execute_reply.started": "2022-09-14T13:42:59.031663Z",
          "shell.execute_reply": "2022-09-14T14:02:34.162122Z"
        },
        "trusted": true,
        "id": "2CFUWSDh8YGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_vector_clf(knn_result):\n",
        "\n",
        "    temp = knn_result[knn_result['Metric'] =='f1_macro']\n",
        "    temp2 = temp.iloc[temp['Best CV Metric Score'].idxmax()].to_frame().T\n",
        "    best_vector = temp2['Vector'].values[0]\n",
        "    best_clf = temp2['Calibrated Estimator'].values[0]\\\n",
        "    \n",
        "    return best_vector, best_clf\n",
        "\n",
        "best_vector, best_clf =  get_best_vector_clf(rfc_result)\n",
        "vis_classification(vector_type = best_vector, estimator = best_clf)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T14:02:34.166581Z",
          "iopub.execute_input": "2022-09-14T14:02:34.16752Z",
          "iopub.status.idle": "2022-09-14T14:02:36.044055Z",
          "shell.execute_reply.started": "2022-09-14T14:02:34.167472Z",
          "shell.execute_reply": "2022-09-14T14:02:36.04254Z"
        },
        "trusted": true,
        "id": "MKVq6NfC8YGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>4.2 <span style='color:red'>|</span>  Dimensionality Reduction </b> <a class=\"anchor\" id=\"4.2\"></a>"
      ],
      "metadata": {
        "id": "zqigr-hW8YGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_temp = rfc_result[rfc_result['Metric'] =='f1_macro']\n",
        "# df_temp['Calibrated Estimator']\n",
        "vector_rfc = df_temp[['Vector','Calibrated Estimator']].set_index('Vector').to_dict()['Calibrated Estimator']\n",
        "vector_rfc"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T14:02:36.046244Z",
          "iopub.execute_input": "2022-09-14T14:02:36.04703Z",
          "iopub.status.idle": "2022-09-14T14:02:36.062816Z",
          "shell.execute_reply.started": "2022-09-14T14:02:36.046981Z",
          "shell.execute_reply": "2022-09-14T14:02:36.060359Z"
        },
        "trusted": true,
        "id": "S1lbY14m8YGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "supported_columns_dict = {}\n",
        "for df_name, df in dataframes.items():\n",
        "    X_train, X_test, y_train, y_test = train_test_split(dataframes[df_name], df_target, test_size=0.2, random_state=random_state_number)\n",
        "\n",
        "    selector = SelectFromModel(estimator=vector_rfc[df_name]).fit(X_train, y_train)\n",
        "    \n",
        "    filter_columns = selector.get_support()\n",
        "    dataframes[df_name] = dataframes[df_name][:, filter_columns]\n",
        "    \n",
        "shape_dim = [] ; df_names = []\n",
        "for df_name, df in dataframes.items():\n",
        "    shape_dim.append(df.shape)\n",
        "    df_names.append(df_name)\n",
        "n_gram_df_dim = pd.DataFrame({'N-Gram Feature Vector':df_names, 'Data Dimension':shape_dim}) \n",
        "n_gram_df_dim"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T14:02:36.066369Z",
          "iopub.execute_input": "2022-09-14T14:02:36.066986Z",
          "iopub.status.idle": "2022-09-14T14:02:40.46484Z",
          "shell.execute_reply.started": "2022-09-14T14:02:36.066952Z",
          "shell.execute_reply": "2022-09-14T14:02:40.463378Z"
        },
        "trusted": true,
        "id": "NYLDPPFR8YGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = n_gram_df_dim['N-Gram Feature Vector'].values\n",
        "b4 = [shape[1] for shape in n_gram_df['Data Dimension'].values]\n",
        "af = [shape[1] for shape in n_gram_df_dim['Data Dimension'].values]\n",
        "\n",
        "x = np.arange(len(labels))  # the label locations\n",
        "width = 0.35  # the width of the bars\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "rects1 = ax.bar(x - width/2, b4, width, label='Before Dimensionality Reduction', color='skyblue')\n",
        "rects2 = ax.bar(x + width/2, af, width, label='After Dimensionality Reduction', color='lime')\n",
        "\n",
        "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
        "ax.set_ylabel('Number Columns')\n",
        "ax.set_title('Before and After Dimensionality Reduction')\n",
        "ax.set_xticks(x, labels)\n",
        "ax.set_xticklabels(ax.get_xticklabels(),rotation=30)\n",
        "ax.legend()\n",
        "\n",
        "ax.bar_label(rects1, padding=3)\n",
        "ax.bar_label(rects2, padding=3)\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T14:02:40.46693Z",
          "iopub.execute_input": "2022-09-14T14:02:40.467484Z",
          "iopub.status.idle": "2022-09-14T14:02:40.873496Z",
          "shell.execute_reply.started": "2022-09-14T14:02:40.467416Z",
          "shell.execute_reply": "2022-09-14T14:02:40.872235Z"
        },
        "trusted": true,
        "id": "xOn2yilQ8YGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'n_neighbors': [5,7,9,11,13,15,17,19,21]}\n",
        "base_estimator = KNeighborsClassifier()\n",
        "knn_result = get_performance(param_grid, base_estimator, dataframes)\n",
        "knn_result"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T14:02:40.875245Z",
          "iopub.execute_input": "2022-09-14T14:02:40.875591Z",
          "iopub.status.idle": "2022-09-14T14:02:50.241125Z",
          "shell.execute_reply.started": "2022-09-14T14:02:40.87556Z",
          "shell.execute_reply": "2022-09-14T14:02:50.239879Z"
        },
        "trusted": true,
        "id": "tJE_ZNOi8YGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_vector, best_clf =  get_best_vector_clf(knn_result)\n",
        "vis_classification(vector_type = best_vector, estimator = best_clf)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T14:02:50.242806Z",
          "iopub.execute_input": "2022-09-14T14:02:50.243646Z",
          "iopub.status.idle": "2022-09-14T14:02:50.896286Z",
          "shell.execute_reply.started": "2022-09-14T14:02:50.24357Z",
          "shell.execute_reply": "2022-09-14T14:02:50.895128Z"
        },
        "trusted": true,
        "id": "eekgWEZH8YGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'max_depth': [None,4,6,7,8,30,32,35],'min_samples_split': [2,3,4,5,35,10,16,20]}\n",
        "base_estimator = DecisionTreeClassifier(random_state=random_state_number)\n",
        "dtc_result = get_performance(param_grid, base_estimator, dataframes)\n",
        "dtc_result"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T14:02:50.89859Z",
          "iopub.execute_input": "2022-09-14T14:02:50.898961Z",
          "iopub.status.idle": "2022-09-14T14:03:36.362717Z",
          "shell.execute_reply.started": "2022-09-14T14:02:50.898927Z",
          "shell.execute_reply": "2022-09-14T14:03:36.361277Z"
        },
        "trusted": true,
        "id": "TPcuzQct8YGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_vector, best_clf =  get_best_vector_clf(dtc_result)\n",
        "vis_classification(vector_type = best_vector, estimator = best_clf)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T14:03:36.364665Z",
          "iopub.execute_input": "2022-09-14T14:03:36.36512Z",
          "iopub.status.idle": "2022-09-14T14:03:37.175028Z",
          "shell.execute_reply.started": "2022-09-14T14:03:36.365085Z",
          "shell.execute_reply": "2022-09-14T14:03:37.174132Z"
        },
        "trusted": true,
        "id": "MoDDQtQc8YGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_result = pd.concat([knn_result, \n",
        "                      dtc_result,\n",
        "                      rfc_result\n",
        "                      ]\n",
        "                     ).reset_index(drop=True)\n",
        "\n",
        "df_result.groupby(['Metric']).max()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T14:03:37.178107Z",
          "iopub.execute_input": "2022-09-14T14:03:37.179654Z",
          "iopub.status.idle": "2022-09-14T14:03:37.210534Z",
          "shell.execute_reply.started": "2022-09-14T14:03:37.179612Z",
          "shell.execute_reply": "2022-09-14T14:03:37.208778Z"
        },
        "trusted": true,
        "id": "W4LXqBtm8YGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>4.3 <span style='color:red'>|</span>  Obtain Best Classifier and Feature Vector </b> <a class=\"anchor\" id=\"4.3\"></a>"
      ],
      "metadata": {
        "id": "rbEg-nt98YGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_result(df_result, metric_score):\n",
        "    df_result_t = df_result[df_result.Metric== 'precision_macro']\n",
        "    precision_macro_df = df_result_t.loc[df_result_t[metric_score].idxmax()].to_frame().T\n",
        "\n",
        "    df_result_t = df_result[df_result.Metric== 'recall_macro']\n",
        "    recall_macro_df = df_result_t.loc[df_result_t[metric_score].idxmax()].to_frame().T\n",
        "    \n",
        "    df_result_t = df_result[df_result.Metric== 'f1_macro']\n",
        "    f1_macro_df = df_result_t.loc[df_result_t[metric_score].idxmax()].to_frame().T\n",
        "\n",
        "    return pd.concat([precision_macro_df,recall_macro_df,f1_macro_df])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T14:03:37.212734Z",
          "iopub.execute_input": "2022-09-14T14:03:37.213533Z",
          "iopub.status.idle": "2022-09-14T14:03:37.224248Z",
          "shell.execute_reply.started": "2022-09-14T14:03:37.213485Z",
          "shell.execute_reply": "2022-09-14T14:03:37.22269Z"
        },
        "trusted": true,
        "id": "gRxiHz_N8YGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_cv_result = get_best_result(df_result, 'Best CV Metric Score')\n",
        "display(best_cv_result)\n",
        "temp = best_cv_result[best_cv_result['Metric'] == 'f1_macro']\n",
        "best_clf = temp['Calibrated Estimator'].values[0]\n",
        "best_vector = temp['Vector'].values[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T14:03:37.227149Z",
          "iopub.execute_input": "2022-09-14T14:03:37.22797Z",
          "iopub.status.idle": "2022-09-14T14:03:37.259402Z",
          "shell.execute_reply.started": "2022-09-14T14:03:37.227922Z",
          "shell.execute_reply": "2022-09-14T14:03:37.25775Z"
        },
        "trusted": true,
        "id": "DSV1vU9Q8YGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_best_result(df_result, 'Test Predict Metric Score')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T14:03:37.261405Z",
          "iopub.execute_input": "2022-09-14T14:03:37.262233Z",
          "iopub.status.idle": "2022-09-14T14:03:37.511895Z",
          "shell.execute_reply.started": "2022-09-14T14:03:37.26219Z",
          "shell.execute_reply": "2022-09-14T14:03:37.510797Z"
        },
        "trusted": true,
        "id": "GDg0L6Zj8YGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>4.4 <span style='color:red'>|</span>  Evaluate on Each Class Labels </b> <a class=\"anchor\" id=\"4.4\"></a>"
      ],
      "metadata": {
        "id": "Ga6kzO128YGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(dataframes[best_vector], df_target, test_size=0.2, \\\n",
        "                                                    random_state=random_state_number)\n",
        "clf = best_clf.fit(X_train, y_train)\n",
        "y_test_pred= clf.predict(X_test)\n",
        "target_names = ['Discharge Summary', 'ENT', 'Neurosurgery']\n",
        "print(classification_report(y_test,y_test_pred,target_names=target_names))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T14:03:37.513505Z",
          "iopub.execute_input": "2022-09-14T14:03:37.51384Z",
          "iopub.status.idle": "2022-09-14T14:03:37.538311Z",
          "shell.execute_reply.started": "2022-09-14T14:03:37.51381Z",
          "shell.execute_reply": "2022-09-14T14:03:37.537052Z"
        },
        "trusted": true,
        "id": "JBJZrZNw8YGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_predict = pd.DataFrame({'Actual Y Test': le.inverse_transform(y_test),'Best Prediction':le.inverse_transform(y_test_pred)})\n",
        "sample_predict.head(20)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-09-14T14:03:37.53982Z",
          "iopub.execute_input": "2022-09-14T14:03:37.540669Z",
          "iopub.status.idle": "2022-09-14T14:03:37.555328Z",
          "shell.execute_reply.started": "2022-09-14T14:03:37.540626Z",
          "shell.execute_reply": "2022-09-14T14:03:37.554408Z"
        },
        "trusted": true,
        "id": "9PaqJDwC8YGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notebook Author**\n",
        "\n",
        "Morris Lee 14/9/2022\n",
        "\n",
        "**References**\n",
        "\n",
        "[1]\tA. Markov, ‚ÄúExample of a statistical investigation of the text of ‚ÄòEugene Onegin‚Äô illustrating the dependence between samples in chain,‚Äù Izvistia Imperatorskoi Akademii Nauk (Bulletin de l‚ÄôAcademie Imp ¬¥ eriale ¬¥ des Sciences de St.-Petersbourg), pp. 153‚Äì162, 1913.\n",
        "\n",
        "[2]\tD. Jurasfky and J. Martin, An introduction to natural language processing, computational linguistics, and speech recognition., 3rd ed. 2021.\n",
        "\n",
        "[3]\tB. J. Marafino, J. M. Davies, N. S. Bardach, M. L. Dean, and R. A. Dudley, ‚ÄúN-gram support vector machines for scalable procedure and diagnosis classification, with applications to clinical free text data from the intensive care unit,‚Äù Journal of the American Medical Informatics Association, vol. 21, no. 5, pp. 871‚Äì875, 2014, doi: 10.1136/amiajnl-2014-002694.\n"
      ],
      "metadata": {
        "id": "N-xyH3cU8YGE"
      }
    }
  ]
}